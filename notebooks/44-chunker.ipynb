{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. In addition, you will try to link a few extracted named entities to real things using wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures\n",
    "* Extract named entities and link them to real things using Wikipedia\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: scikit-learn\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As annotated data and annotation scheme, you will use the data available from [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/).\n",
    "* Download both the training and test sets and decompress them.\n",
    "* Local copies are also available here: [train.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt) and [test.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt)\n",
    "* Read the description of the CoNLL 2000 task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the paths to load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'corpus/conll2000/train.txt'\n",
    "test_file = 'corpus/conll2000/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline performance corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs.\n",
    "\n",
    "You will implement the baseline proposed by the organizers of the\n",
    "        <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\">CoNLL 2000 shared task</a>, Sect. <i>Results</i>.\n",
    "1. Read it;\n",
    "2. In the report you will tell what do you think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect all the parts of speech and we count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 30147,\n",
       " 'IN': 22764,\n",
       " 'DT': 18335,\n",
       " 'VBZ': 4648,\n",
       " 'RB': 6607,\n",
       " 'VBN': 4763,\n",
       " 'TO': 5081,\n",
       " 'VB': 6017,\n",
       " 'JJ': 13085,\n",
       " 'NNS': 13619,\n",
       " 'NNP': 19884,\n",
       " ',': 10770,\n",
       " 'CC': 5372,\n",
       " 'POS': 1769,\n",
       " '.': 8827,\n",
       " 'VBP': 2868,\n",
       " 'VBG': 3272,\n",
       " 'PRP$': 1881,\n",
       " 'CD': 8315,\n",
       " '``': 1531,\n",
       " \"''\": 1493,\n",
       " 'VBD': 6745,\n",
       " 'EX': 206,\n",
       " 'MD': 2167,\n",
       " '#': 36,\n",
       " '(': 274,\n",
       " '$': 1750,\n",
       " ')': 281,\n",
       " 'NNPS': 420,\n",
       " 'PRP': 3820,\n",
       " 'JJS': 374,\n",
       " 'WP': 529,\n",
       " 'RBR': 321,\n",
       " 'JJR': 853,\n",
       " 'WDT': 955,\n",
       " 'WRB': 478,\n",
       " 'RBS': 191,\n",
       " 'PDT': 55,\n",
       " 'RP': 83,\n",
       " ':': 1047,\n",
       " 'FW': 38,\n",
       " 'WP$': 35,\n",
       " 'SYM': 6,\n",
       " 'UH': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cnt = count_pos(train_corpus)\n",
    "pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the chunk distribution for each part of speech. You will use the training file to derive the distribution and you will store the results in a dictionary. Below, you have an excerpt of the expected results:\n",
    "```\n",
    "{'JJR':\n",
    "{'I-ADVP': 17, 'I-ADJP': 45, 'I-NP': 204, 'B-ADVP': 63,\n",
    "'B-PP': 2, 'B-ADJP': 111, 'B-NP': 382, 'B-VP': 2,\n",
    "'I-VP': 11, 'O': 16},\n",
    "'CC':\n",
    "{'B-ADVP': 3, 'O': 3676, 'I-VP': 104, 'B-CONJP': 6,\n",
    "'I-ADVP': 30, 'I-UCP': 2, 'I-PP': 24, 'I-ADJP': 26,\n",
    "'I-NP': 1409, 'B-ADJP': 2, 'B-NP': 18, 'B-PP': 70,\n",
    "'I-PRT': 1, 'B-VP': 1},\n",
    "'NN':\n",
    "{'B-LST': 2, 'I-INTJ': 2, 'B-ADVP': 38, 'O': 37,\n",
    "'I-ADVP': 11, 'B-INTJ': 1, 'I-UCP': 2, 'B-UCP': 2,\n",
    "'I-VP': 77, 'B-PRT': 2, 'I-ADJP': 41, 'I-NP': 24456,\n",
    "'B-ADJP': 44, 'B-NP': 5160, 'B-PP': 15, 'B-VP': 257},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def increment_dic(dic, item):\n",
    "    if item in dic:\n",
    "        dic[item] += 1\n",
    "    else:\n",
    "        dic[item] = 1\n",
    "\n",
    "chunk_dist = {}\n",
    "\n",
    "for sentence in train_corpus:\n",
    "    for dic_word_categoriser in sentence:\n",
    "        pos = dic_word_categoriser[\"pos\"]\n",
    "        chunk = dic_word_categoriser[\"chunk\"]\n",
    "        \n",
    "        if pos in chunk_dist:\n",
    "             increment_dic(chunk_dist[pos], chunk)   \n",
    "        else:\n",
    "            chunk_dist[pos] = {}\n",
    "            increment_dic(chunk_dist[pos], chunk)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-NP': 5160,\n",
       " 'I-NP': 24456,\n",
       " 'B-VP': 257,\n",
       " 'B-ADJP': 44,\n",
       " 'B-ADVP': 38,\n",
       " 'O': 37,\n",
       " 'B-PP': 15,\n",
       " 'I-ADVP': 11,\n",
       " 'I-ADJP': 41,\n",
       " 'I-VP': 77,\n",
       " 'B-INTJ': 1,\n",
       " 'B-LST': 2,\n",
       " 'B-UCP': 2,\n",
       " 'I-UCP': 2,\n",
       " 'B-PRT': 2,\n",
       " 'I-INTJ': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dist['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the POS-chunk associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each part of speech, select the best association. In the example above, you will have (NN, I-NP) as it is the most frequent. You will store the results in a dictionary that you will call `pos_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pos_chunk = {}\n",
    "\n",
    "for chunk_item in chunk_dist.items():\n",
    "    pos = chunk_item[0]\n",
    "    chunk_dic = chunk_item[1]\n",
    "    \n",
    "    most_common_chunk_to_freq = max(chunk_dic.items(), key=lambda chunk_to_freq: chunk_to_freq[1])\n",
    "    most_common_chunk = most_common_chunk_to_freq[0]\n",
    "    \n",
    "    pos_chunk[pos] = most_common_chunk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-NP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_chunk['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the resulting associations, apply your chunker to the test file. You will write a `predict(model, corpus)` function, where `model` will be your associations and `corpus`, the test corpus. You will format the test corpus as a dictionary, where you will add a `pchunk` key for each row with a value that will correspond to the predicted chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict(model, corpus):\n",
    "    prediction_corpus = corpus.copy()\n",
    "    \n",
    "    for sentence in prediction_corpus:\n",
    "        for dic_word_categories in sentence:\n",
    "            \n",
    "            pos = dic_word_categories['pos']\n",
    "            \n",
    "            dic_word_categories['pchunk'] = pos_chunk[pos]\n",
    "            \n",
    "    return prediction_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the groups. You should have added a `pchunk` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_corpus = predict(pos_chunk, test_corpus)\n",
    "predicted_test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the baseline with the tag accuracy: the percentage of words that receive the correct tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the predicted chunk accuracy\n",
    "    :param predicted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_cnt = 0\n",
    "    correct = 0\n",
    "    for sentence in predicted:\n",
    "        for row in sentence:\n",
    "            word_cnt += 1\n",
    "            if row['chunk'] == row['pchunk']:\n",
    "                correct += 1\n",
    "    return correct / word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = eval(predicted_test_corpus)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoNLL evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very misleading as it is biased by the most frequent tags. It is not a good way to evaluate chunking. Instead, CoNLL computes the F1 score of all the chunks with a specific evaluation script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the CoNLL evaluation script, you will store your results in an output file that has four columns. The three first columns will be the input columns from the test file: \n",
    "* word, \n",
    "* part of speech, and \n",
    "* gold-standard chunk. \n",
    "\n",
    "You will append the predicted chunk as the 4th column. Your output file should look like the excerpt below:\n",
    "```\n",
    "Rockwell NNP B-NP I-NP\n",
    "International NNP I-NP I-NP\n",
    "Corp. NNP I-NP I-NP\n",
    "'s POS B-NP B-NP\n",
    "Tulsa NNP I-NP I-NP\n",
    "unit NN I-NP I-NP\n",
    "said VBD B-VP B-VP\n",
    "it PRP B-NP B-NP\n",
    "```\n",
    "The separator is the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a `save_results(output_dict, keys, output_file)` function, where the keys will be `['form', 'pos', 'chunk', 'pchunk']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dict, keys, output_file):\n",
    "    f_out = open(output_file, 'w')\n",
    "    # We write the word (form), part of speech (pos),\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\n",
    "    for sentence in output_dict:\n",
    "        for row in sentence:\n",
    "            for key in keys:\n",
    "                f_out.write(row[key] + ' ')\n",
    "            f_out.write('\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(predicted_test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 evaluation script will use these two last columns, chunk and predicted chunk, to compute the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your results, you have two options:\n",
    "1. Use the original conlleval script here:  <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\"><tt>conlleval.txt</tt></a>.\n",
    "2. Use a Python translation of it. \n",
    "\n",
    "You will use the second option and you will describe the results you obtained in your report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Python translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the script with:\n",
    "```\n",
    "pip install conlleval\n",
    "```\n",
    "from https://github.com/kaniblu/conlleval\n",
    "and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conlleval\n",
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "baseline_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770671072299583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The official script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to double-check your results with the original CoNLL script. It is more complex to use however:\n",
    "* <tt>conlleval.txt</tt> is the official CoNLL Perl script. It expects the two last columns of the test set to be the manually assigned chunk (gold standard) and the predicted chunk.\n",
    "* <tt>conlleval.txt</tt> was written for Unix and if you run Windows, you will have to use a terminal command. In the File menu of the notebook, select New and then Terminal.\n",
    "* Start it like this: ` $ conlleval.txt <out` where the `out` file contains both the gold and predicted chunk tags. `conlleval.txt` is a Perl script.\n",
    "* Perl is installed on most Unix distributions. If it is not installed on your machine, you need to install it. Make also sure that you have the execution rights. Otherwise change them with: `$ chmod +x conlleval.txt`\n",
    "* The `conlleval.txt` script expects the new lines to be `\\n` as in Unix. If you run your Python program on Windows, your new lines will be `\\r\\n`. To have the correct new lines, add this parameter to `open()`: `newline='\\n’` like this: `f_out = open('out', ‘w’, newline='\\n’)`\n",
    "* The complete description of the CoNLL 2000 evaluation script is available here: [https://www.clips.uantwerpen.be/conll2000/chunking/output.html](https://www.clips.uantwerpen.be/conll2000/chunking/output.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: A first ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply and explore a machine-learning program.\n",
    "\n",
    "The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a window of five words around the chunk tag to identify, $c_i$. They built a feature vector consisting of:\n",
    "1. The values of the five words in this window: $w_{i-2}, w_{i-1}, w_{i}, w_{i+1}, w_{i+2}$\n",
    "2. The values of the five parts of speech in this window: $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}, t_{i+2}$\n",
    "3. The values of the two previous chunk tags in the first part of the window: $c_{i-2}, c_{i-1}$\n",
    "\n",
    "The two last parameters (3.) are said to be dynamic because the program computes them at run-time. Read [Kudoh and Matsumoto's paper](http://www.clips.uantwerpen.be/conll2000/pdf/14244kud.pdf) and the [Yamcha](http://www.chasen.org/~taku/software/yamcha/) software site.\n",
    "\n",
    "You will start with a given code that uses the two first sets of features (1. and 2.) and add yourself the last one (3.) to improve the performance of your chunker. Kudoh and Matsumoto trained a classifier based on support vector machines. You will use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import requests\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first function to extract features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_static(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_static(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_static(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the window and the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the corpus and format it as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict, y = extract_features_static(train_corpus, w_size, feature_names)\n",
    "X_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'B-PP']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the sentences and create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-NP', 'I-NP'], dtype='<U7')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = classifier.predict(X_test)\n",
    "y_test_predicted[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the predicted chunks to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index sould be equal to the length of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47377"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inx)\n",
    "len(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "simple_ml_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156857035897009"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question on the ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the feature vector that corresponds to the <tt>ml_chunker.py</tt> program? Is it the same Kudoh\n",
    "    and Matsumoto used in their experiment?\n",
    "2. What is the performance of the chunker?\n",
    "3. Remove the lexical features (the words) from the feature vector and measure the performance. You should\n",
    "    observe a decrease.\n",
    "4. What is the classifier used in the program? \n",
    "5. As an optional task, you may try two other classifiers from sklearn and measure their performance: decision trees, perceptron, support vector machines, etc. Be aware that support vector machines take a long time to train: up to one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: Adding all the features from Kudoh and Matsumoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the feature vector used in the previous section with the two dynamic features, $c_{i-2}, c_{i-1}$, and train a new model. You will need to write a new `extract_features_sent_dyn` and `predict` functions. \n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with logistic regression and the default parameters from sklearn, i.e. `linear_model.LogisticRegression()`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A frequent mistake in the labs** is to use the gold-standard chunks from the test set. Be aware that  when you predict the test set, you do not know the dynamic features in advance and you must  not use the ones from the test file. You will use the two previous chunk tags that you have predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to reach a global F1 score of 92 to pass this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write your code here\n",
    "def extract_features_sent_dyn(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        for j in range(w_size):\n",
    "            x.append(padded_sentence[i + j]['chunk'])\n",
    "            \n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dyn(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_dyn(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_dyn = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                     'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2', 'chunk_n2',\n",
    "                     'chunk_n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict, y = extract_features_dyn(train_corpus, w_size, feature_names_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'BOS'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'confidence',\n",
       "  'word_n1': 'in',\n",
       "  'word': 'the',\n",
       "  'word_p1': 'pound',\n",
       "  'word_p2': 'is',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'VBZ',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'B-PP'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now vectorize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "vec = DictVectorizer(sparse = True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will finally predict the test set. We load the corpus again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the static features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{X}\\_{\\textrm{dict}}$ is incomplete. For the prediction, we need to reinject dynamically the two previously predicted tags to have the full feature vector. Write this code here. \n",
    "\n",
    "This part is probably the most difficult of the lab. You may want to write it first for one sentence, and then for the test corpus. The prediction will take a longer time and you may want to include a progress bar with this snippet: \n",
    "```\n",
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "X_temp = X_test_dict.copy()\n",
    "\n",
    "i = 1\n",
    "y_internal = ['BOS', 'BOS']\n",
    "\n",
    "for word in X_temp:\n",
    "    chunk2 = y_internal[i - 1]\n",
    "    chunk1 = y_internal[i]\n",
    "\n",
    "    word['chunk_n2'] = chunk2\n",
    "    word['chunk_n1'] = chunk1\n",
    "\n",
    "    y_word = classifier.predict(vec.transform(word))\n",
    "    y_internal += list(y_word)\n",
    "    y_test_predicted_dyn += list(y_word)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    if word['word_p1'] == 'EOS':\n",
    "        i = 1\n",
    "        y_internal = ['BOS', 'BOS']\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'I-NP']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_dyn[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted_dyn[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9263263766658284"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "improved_ml_score = res['overall']['chunks']['evals']['f1']\n",
    "improved_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional task, you can try to improve the score with beam search. If you know this technique, apply it using the probability output of logistic regression.\n",
    "\n",
    "With the same classifier and a beam diameter of 5, your teacher obtained 92.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now collect all the named entities from the training set, defined as NP chunks and starting with a `NNP` (proper noun) or a `NNPS` (proper noun, plural) tag. As an example, in the first sentence of `train_corpus`, you will extract `('September', )` and `('July', 'and', 'August')`. You will set all the tuples in a set that you will call `ne_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'This', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       " {'form': 'increased', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'risk', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'government', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'being', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       " {'form': 'forced', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       " {'form': 'increase', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'base', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'rates', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       " {'form': '16', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       " {'form': '%', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'their', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       " {'form': 'current', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': '15', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       " {'form': '%', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'level', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       " {'form': 'defend', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       " {'form': 'economists', 'pos': 'NNS', 'chunk': 'B-NP'},\n",
       " {'form': 'and', 'pos': 'CC', 'chunk': 'O'},\n",
       " {'form': 'foreign', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       " {'form': 'exchange', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'market', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'analysts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       " {'form': 'say', 'pos': 'VBP', 'chunk': 'B-VP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a two-pass procedure. For each sentence of the corpus:\n",
    "1. In the first pass, you will collect the start indices of the noun groups which are also proper nouns. For the first sentence, it will result in the list `[16, 30]`;\n",
    "2. In the second pass, you will collect the segments, starting at each index. For the first sentence, it will result in the tuples `('September',)`and `('July', 'and', 'August')`\n",
    "\n",
    "Should you have a better solution, please use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-63-61f0aa436cfd>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-61f0aa436cfd>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "ne_list = []\n",
    "\n",
    "for sentence in train_corpus:\n",
    "    i = 0\n",
    "    ne = []\n",
    "    indencies = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        pos = word['pos']\n",
    "        chunk = word['chunk']\n",
    "        \n",
    "        if (pos == 'NNP' or pos == 'NNPS') and chunk == \"B-NP\":\n",
    "            indencies.append(i)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    for j in range(len(indencies)):\n",
    "        index = indencies[j];\n",
    "        print(\"len\")\n",
    "        print(len(indencies))\n",
    "        ne.append(sentence[index]['form'])\n",
    "        index += 1;\n",
    "\n",
    "        go = True\n",
    "        \n",
    "        if index < len(sentence):\n",
    "            try:\n",
    "                while go:\n",
    "                    print(index)\n",
    "\n",
    "                    chunk = sentence[index]['chunk']\n",
    "\n",
    "                    if  chunk == \"I-NP\":\n",
    "                        ne.append(sentence[index]['form'])\n",
    "                    else:\n",
    "                        go = False\n",
    "\n",
    "                    index += 1;\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_set = set(ne_list)\n",
    "len(ne_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ne_set)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a small set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the subsequent experiments faster, you will limit the dataset to the entities starting with letter _K_. I chose this letter, because it corresponded to one of the smallest sets. You will call the resulting set: `ne_small_set`. Feel free to use the full set after you have completed this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "ne_small_set = list(filter(lambda tup: tup[0][0] == 'K', ne_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_small_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a simple method to find the named entities from the previous exercise in Wikipedia and Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, look at a few entities in your set and find:\n",
    "1. a few entities that you think are in wikipedia, \n",
    "2. entities that will not be in wikipedia, and \n",
    "3. entities that you think are ambiguous: An entity that may correspond to two or more things. \n",
    "\n",
    "You will describe your findings in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to lookup entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the function below and try to understand what it means. You will describe it in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_lookup(ner, base_url='https://en.wikipedia.org/wiki/'):\n",
    "    try:\n",
    "        url_en = base_url + ' '.join(ner)\n",
    "        html_doc = requests.get(url_en).text\n",
    "        parse_tree = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        entity_id = parse_tree.find(\"a\", {\"accesskey\": \"g\"})['href']\n",
    "        head_id, entity_id = os.path.split(entity_id)\n",
    "        return entity_id\n",
    "    except:\n",
    "        pass\n",
    "        # print('Not found in: ', base_url)\n",
    "    entity_id = 'UNK'\n",
    "    return entity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to run the lookup and keep the resolved entities (only the resolved entities). You will call it `ne_ids_en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "ne_ids_en = []\n",
    "\n",
    "for tupel in ne_small_set:\n",
    "    data = wikipedia_lookup(tupel)\n",
    "    \n",
    "    \n",
    "    if data != 'UNK':\n",
    "        ne_ids_en.append((tupel, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_ids_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, entities need a confirmation. You will apply the resolution with the Swedish wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "ne_ids_sv = []\n",
    "\n",
    "for tupel in ne_small_set:\n",
    "    data = wikipedia_lookup(tupel, \"https://sv.wikipedia.org/wiki/\")\n",
    "    \n",
    "    \n",
    "    if data != 'UNK':\n",
    "        ne_ids_sv.append((tupel, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_ids_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the intersection of the two sets. You will assign it to a list that you will sort and that you will call: `confirmed_ne_en_sv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "confirmed_ne_en_sv = []\n",
    "\n",
    "ne_ids_en = sorted(ne_ids_en, key=lambda tupel: -int(tupel[1][1:]))\n",
    "ne_ids_sv = sorted(ne_ids_sv, key=lambda tupel: -int(tupel[1][1:]))\n",
    "\n",
    "for entity_en in ne_ids_en:\n",
    "    for entity_sv in ne_ids_sv:\n",
    "        \n",
    "        if entity_sv[1] == entity_en[1]:\n",
    "                confirmed_ne_en_sv.append(entity_en)\n",
    "        elif int(entity_sv[1][1:]) < int(entity_en[1][1:]):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_ne_en_sv[4:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first items in your list should look like:\n",
    "```\n",
    "[(('KIM',), 'Q224736'),\n",
    " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"vi1146ol-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"44-chunker.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the baseline score, the improved machine-learning score, and the confirmed entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'baseline_score': baseline_score,\n",
    "                    'improved_ml_score': improved_ml_score,\n",
    "                    'confirmed_ne_en_sv': confirmed_ne_en_sv})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 4\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String\n",
    "            Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018)\n",
    "            and you will outline the main differences between their system and yours. A LSTM is a type of\n",
    "            recurrent neural network, while CRF is a sort of beam search. You will tell the performance\n",
    "            they reach on the corpus you used in this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
